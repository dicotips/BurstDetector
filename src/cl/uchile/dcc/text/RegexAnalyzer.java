package cl.uchile.dcc.text;

import cl.uchile.dcc.utils.PropertiesTD;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.util.*;
import java.util.regex.Pattern;

import com.atilika.kuromoji.ipadic.Token;
import com.atilika.kuromoji.ipadic.Tokenizer;

/**
 * This class cleans text, tokenizes it and generate NGrams. It also discards 
 * stopwords.
 * 
 * @author      <a href="mailto:jguzman@dcc.uchile.cl">Jheser Guzman</a>
 * @version     1.0
 * @since       2016-08-17
 */
public class RegexAnalyzer implements TextAnalyzer {
  private static LanguagesAnalizer nlp;
  
  private Set<String> stopwords;
  private StringTokenizer symTokenizer;

  /**
   * Constructor that initializes the instance using the parameters of
   * the _setup.txt file.
   * @param prop Parameters of the _setup.txt file.
   */
  public RegexAnalyzer(PropertiesTD prop) {
    
    if (nlp == null){
      nlp = new LanguagesAnalizer(prop);
    }
    //Carga stopwords (Conjunto)
    stopwords = new HashSet<>();
    File dir = new File(prop.StopWords_Data);

    String[] children = dir.list();
    if (children == null) {
      // Either dir does not exist or is not a directory
    } 
    else {
      for (int i = 0; i < children.length; i++) {
        // Get filename of file or directory
        if (children[i].endsWith(".txt")) {
          BufferedReader br = null;
          try {
            String line = null;
            br = new BufferedReader(new FileReader(new File(prop.StopWords_Data +"/"+ children[i])));
            while ((line = br.readLine()) != null) {
              stopwords.add(line.trim().toLowerCase());
            }
            br.close();
          } 
          catch (FileNotFoundException ex) {
            ex.printStackTrace();
          } 
          catch (IOException ex) {
            ex.printStackTrace();
          }
        }
      }
    }
  }

  /**
   * Method that cleans the text replacing accents and other add-symbols. It
   * tokenizes the given text using the specified language.
   * @param text  Text to process and tokenize.
   * @param lang  Language of the text.
   * @return Map with tokens and their frequencies.
   */
  @Override
  public Map<String, Integer> analyzeText(String text, String lang) {
    //Extrae URLS
    text = removeUrls(text);

    text = text.toLowerCase();
    text = text.replace("[video]","");
    text = text.replace('Ã¡', 'a');
    text = text.replace('Ã©', 'e');
    text = text.replace('Ã­', 'i');
    text = text.replace('Ã³', 'o');
    text = text.replace('Ãº', 'u');
    text = text.replace('Ã¤', 'a');
    text = text.replace('Ã«', 'e');
    text = text.replace('Ã¯', 'i');
    text = text.replace('Ã¶', 'o');
    text = text.replace('Ã¼', 'u');
    text = text.replace('Ã ', 'a');
    text = text.replace('Ã¨', 'e');
    text = text.replace('Ã¬', 'i');
    text = text.replace('Ã²', 'o');
    text = text.replace('Ã¹', 'u');
    text = text.replace('Ã¢', 'a');
    text = text.replace('Ãª', 'e');
    text = text.replace('Ã®', 'i');
    text = text.replace('Ã´', 'o');
    text = text.replace('Ã»', 'u');
    text = text.replace('Ã£', 'a');
    text = text.replace('Ã¥', 'a');
    text = text.replace('Ãµ', 'o');
    text = text.replace('Ã§', 'c');
    text = text.replace('Ã½', 'y');
    text = text.replace('á»³', 'y');
    text = text.replace('Å·', 'y');
    text = text.replace('Ã¿', 'y');
    text = text.replace("Ã¦", "ae");
    text = text.replace("Ã±", "n");
    //text = text.replaceAll("[^A-Za-z0-9#@_-]", " ");

    Map<String, Integer> terms = new HashMap<>();
    
    if(nlp.containsLanguage(lang)){
      Map<String, Integer>  nlp_tokens = nlp.analyzeText(text, lang);
      
      for (String nlp_token : nlp_tokens.keySet()) {
        String token = nlp_token.replaceAll("[ï¼»ï¼½ã€ã€ã€ˆã€‰ã€Šã€‹ã€”ã€•ã€–ã€—ï¼ˆï¼‰ã€ã€‘ï¼Œï¼šï¼›ï¼Ÿï¼ã€€ \\ÂªÂº!|\"Â·$%&Â¬/()=?Â¡Â¿\\[^+*\\]{},;.:-_â€œ~â€˜Â°â€\t\n<>\r'ã€‚ã€ã€Œã€,ï¹ï¹‚â€¦]", "").trim();
        
        if (token.length() > 1 && !token.matches("\\s+") && !stopwords.contains(token) && !isNumeric(token)) {
          if (terms.containsKey(token)) {
            terms.put(token, terms.get(token) + 1);
          } else {
            terms.put(token, 1);
          }
        }
      }
    }
    else{
      //Tokenizazinyt JA language
      if(lang == "ja"){
        Tokenizer tokenizer = new Tokenizer() ;
        List<Token> tokens = tokenizer.tokenize(text);
        String aux = "";
        for (Token token : tokens) {
          aux += " " + token.getSurface();
        }
        text = aux.trim();
      }
      
      //Tokenizacion process
      symTokenizer = new StringTokenizer(text, "ï¼»ï¼½ã€ã€ã€ˆã€‰ã€Šã€‹ã€”ã€•ã€–ã€—ï¼ˆï¼‰ã€ã€‘ï¼Œï¼šï¼›ï¼Ÿï¼ã€€ \\ÂªÂº!|\"Â·$%&Â¬/()=?Â¡Â¿[^+*]{},;.:-_â€œ~â€˜Â°â€\t\n<>\r'ã€‚ã€ã€Œã€,ï¹ï¹‚â€¦");

      while (symTokenizer.hasMoreTokens()) {
        String token = symTokenizer.nextToken();
        token = removeDigits(token);

        if (token.length() > 1 && !token.matches("\\s+") && !stopwords.contains(token) && !isNumeric(token)) {
          if (terms.containsKey(token)) {
            terms.put(token, terms.get(token) + 1);
          } else {
            terms.put(token, 1);
          }
        }
      }
    }
    return terms;
  }
  
  /**
   * Verifies if the string is a number.
   * @param str text to analyze.
   * @return TRUE=Is number; FALSE= Is not a number.
   */
  private boolean isNumeric(String str)
  {  
    try  
    {  
      double d = Double.parseDouble(str);  
    }  
    catch(NumberFormatException nfe)  
    {  
      return false;  
    }  
    return true;  
  }
  
  /**
   * Remove digits from the given text.
   * @param token Text to process removing digits.
   * @return Text without digits.
   */
  private String removeDigits(String token) {
    StringBuilder filteredString = new StringBuilder();

    for (int i = 0; i < token.length(); i++) {
      char c = token.charAt(i);
      if (Character.isLetter(c) || Character.isDigit(c) || c=='#' || c=='@') {
        filteredString.append(token.substring(i));
        break;
      }
    }

    return filteredString.toString();
  } 
  
  /**
   * Remove URLS from the given text.
   * @param  text Text to process removing URLs.
   * @return Text without URLS.
   */
  private String removeUrls(String text) {
    Pattern pattern = Pattern.compile("(https?|ftp|gopher|telnet|file|Unsure|http)://[\\S]*", Pattern.CASE_INSENSITIVE);
    String newtext = pattern.matcher(text).replaceAll("");
    return newtext;
  }
  
  /**
   * NGrams generator from 1 to n_gram number. It generates NGrams per character.
   * @param n_gram  Up to ngrams size.
   * @param text    Text to generate NGrams per characters.
   * @return List of Ngrams.
   */
  private List<String> nGram( int n_gram, String text){
    List<String> grams = new ArrayList<>();
    
    for (int i = 0; i < text.length()-n_gram+1; i++) {
      String token = text.substring(i, i+n_gram).trim();
      if(token.length() != n_gram)
        continue;
      
      grams.add(token);
    }
    return grams;
  }
  
  /**
   * NGrams generator per tokens.
   * @param text Text to analyze.
   * @return List of Ngrams based on tokens, and their frequencies.
   */
  @Override
  public Map<String, Integer> analyzeText_NGram(String text) {
    //Extrae URLS
    text = removeUrls(text);

    text = text.toLowerCase();
    text = text.replace("[video]","");
    text = text.replace('Ã¡', 'a');
    text = text.replace('Ã©', 'e');
    text = text.replace('Ã­', 'i');
    text = text.replace('Ã³', 'o');
    text = text.replace('Ãº', 'u');
    text = text.replace('Ã¤', 'a');
    text = text.replace('Ã«', 'e');
    text = text.replace('Ã¯', 'i');
    text = text.replace('Ã¶', 'o');
    text = text.replace('Ã¼', 'u');
    text = text.replace('Ã ', 'a');
    text = text.replace('Ã¨', 'e');
    text = text.replace('Ã¬', 'i');
    text = text.replace('Ã²', 'o');
    text = text.replace('Ã¹', 'u');
    text = text.replace('Ã¢', 'a');
    text = text.replace('Ãª', 'e');
    text = text.replace('Ã®', 'i');
    text = text.replace('Ã´', 'o');
    text = text.replace('Ã»', 'u');
    text = text.replace('Ã£', 'a');
    text = text.replace('Ã¥', 'a');
    text = text.replace('Ãµ', 'o');
    //text = text.replace('Ã§', 'c');
    text = text.replace('Ã½', 'y');
    text = text.replace('á»³', 'y');
    text = text.replace('Å·', 'y');
    text = text.replace('Ã¿', 'y');
    //text = text.replace("Ã¦", "ae");
    //text = text.replace("Ã±", "n");
    //text = text.replaceAll("[^A-Za-z0-9#@_-]", " ");
    text = text.replaceAll("\\t\\n<>\\r", " ");

    Map<String, Integer> terms = new HashMap<>();
    
    for (String token : nGram(1, text)) {
      if (terms.containsKey(token)) {
        terms.put(token, terms.get(token) + 1);
      } else {
        terms.put(token, 1);
      }
    }
    
    for (String token : nGram(2, text)) {
      if (terms.containsKey(token)) {
        terms.put(token, terms.get(token) + 1);
      } else {
        terms.put(token, 1);
      }
    }
    
    return terms;
  }

  /**
   * PowerSet Generator per tokens
   * @param originalSet List of tokens.
   * @return List of combinations of tokens grouped from 1 to ...
   */
  @Override
  public Set<Set<String>> powerSet(Set<String> originalSet) {
    Set<Set<String>> sets = new HashSet<>();
    if (originalSet.isEmpty()) {
    	sets.add(new HashSet<String>());
    	return sets;
    }
    List<String> list = new ArrayList<String>(originalSet);
    String head = list.get(0);
    Set<String> rest = new HashSet<String>(list.subList(1, list.size())); 
    for (Set<String> set : powerSet(rest)) {
    	Set<String> newSet = new HashSet<String>();
    	newSet.add(head);
    	newSet.addAll(set);
    	sets.add(newSet);
    	sets.add(set);
    }		
    return sets;
  }
  
  /**
   * This is the main method tests the use of the RegexAnalyzer for 
   * tokenizing text in diferent languages and their NGrams.
   * 
   * @param  args Nothing
   */
  public static void main(String args[]) {
    PropertiesTD PropTD = new PropertiesTD("/Users/dicotips/Dropbox/Research_SourceCode/Twitter_Crawler/_setup.txt");
    TextAnalyzer analyzer = new RegexAnalyzer(PropTD);

    String str1  = "[Video] Llamarada solar amenaza @Jheser #filio con interrumpir comunicaciones satelitales http://t.co/SUaxNBB";
    String str2  = "Revisa la @SecciÃ³n #OpiniÃ³n 49ers revisa: Perdona-vidas...a propÃ³sito de los dichos de mi amigo Enrique Correa, por Luis Alvarado";
    String str3  = ":)https://ow.ly/i/d0JK. ..http://ow.ly/i/d0JK http";
    String str4  = "Ganti nick aja deh ehehehe Æª(Ë˜âŒ£Ë˜ )â”Æª(Ë˜âŒ£Ë˜)Êƒâ”Œ( Ë˜âŒ£Ë˜)Êƒ";
    String str5  = "Meeeeeee ! RT @missLOVElace_: who wants my 80,000 tweet ?";
    String str6  = "@croqui9 ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼";    
    String str7  = "@5HVoteStats HARMOS GO CRAZY 2016 #harmonizers #iheartawards #BestFanArmy";
    String str8  = "åº”æœ‰å°½æœ‰çš„ä¸°å¯Œé€‰æ‹©å®šå°†ä¸ºæ‚¨çš„æ—…ç¨‹å¢æ·»æ— æ•°çš„èµå¿ƒä¹äº‹";
    String str9  = "æœ¬å§‘å¨˜ä¸çˆ½ï¼Œï¼»alohaï¼½alohaã€alohaã€ã€ˆalohaã€‰ã€Šalohaã€‹ã€”alohaã€•ã€–alohaã€—ï¼ˆalohaï¼‰ã€alohaã€‘æ‰€ä»¥æ±ºå®šæ”¤ç‰Œ!!!!!!! ç°¡å–®çš„äº¤ç­äººé¸å¯ä»¥æ‹–äº†2.3å€‹ç¦®æ‹œç„¡æ³•æ±ºå®šæ˜¯åœ¨æä»€éº¼é¬¼???!!! https://t.co/Qy78jCWmis";
    String str10 = "ãŠå¯¿å¸ãŒé£Ÿã¹ãŸã„ã€‚";
    
    List<String> str = new ArrayList<>();
    str.add("rt @yien093x: âŒâ â•»áŸï½¡ ğŸ•¸á¢Ë–æ©-æ®µå®œ, 93Â° ğŸ”­ËŸâ•»Â¯âŒ‡ Â°Ë–Í¯â™¡Ì·Ë–04.09%ğŸ“½â¸—â®rapperğŸŒªâ¯ â€¤ â•·ó¾Ÿ—Â·â€›@ËŸjypcuteboys â‘ˆâªó¾ €â«Ë–âŒâ•·");
    str.add("âŒâ â•»áŸï½¡ ğŸ•¸á¢Ë–æ©-æ®µå®œ, 93Â° ğŸ”­ËŸâ•»Â¯âŒ‡ Â°Ë–Í¯â™¡Ì·Ë–04.09%ğŸ“½â¸—â®rapperğŸŒªâ¯ â€¤ â•·ó¾Ÿ—Â·â€›@ËŸjypcuteboys â‘ˆâªó¾ €â«Ë–âŒâ•·");
    str.add("åŠå¸¶çŸ­è£™â†¬ ó¾“•ó¾“‘ó¾“•ó¾“‘ó¾“•ó¾“‘ó¾“•ó¾“‘ó¾“•ó¾“‘ó¾“•ó¾“‘ó¾“• â†±â†± æ€éº¼ç©¿æ€éº¼q â†°â†° æ­¡è¿ç¾ç¾çš„å¦³å€‘å’Œæˆ‘å€‘åˆ†äº«å¦³çš„ â˜¾â˜¾funkylookç©¿æ­â˜½â˜½ ç©¿æ­åˆ†äº«ï¼š å¼µèŠ³ç‘œ");
    str.add("ó¾”—ó¾”—å…é‹å…è²»ç´¢å–é«”é©—åŒ…ó¾”—ó¾”— ó¾­¹ó¾­¹ó¾­¹ç´¢å–æ–¹å¼èªªæ˜ó¾­¹ó¾­¹ó¾­¹ âœ…ç›´æ¥é»é¸åŠ å…¥footpureé‹èœœç²‰å®˜æ–¹line>> âœ…è«‹æ–¼lineç•™è¨€å‘ŠçŸ¥å¯„é€ä»¥ä¸‹è³‡æ–™ï¼š...");
    str.add("#é›†é‚®# è¿‘æ—¥æ”¶åˆ°ï¼š1ã€2ã€6ã€è¥„é˜³é‚®å‹æ¨å¥³å£«åœ¨å—æ¼³å¿é•¿åªé•‡å¯„æ¥çš„â€œç›¸æ€é¸Ÿâ€åŸåœ°å°&æé™ç‰‡ä»¥åŠâ€œå¥¥è¿â€é¦–æ—¥å°ï¼›3ã€éšå·é‚®å‹å‘¨å…ˆç”Ÿåœ¨è¥„é˜³â€œç›¸æ€é¸Ÿâ€åŸåœ°å°ï¼›4ã€5ã€è¥„é˜³é‚®å‹åˆ˜å…ˆç”Ÿåœ¨â€œç›¸æ€é‚®å±€â€å¯„æ¥çš„â€œç›¸æ€é¸Ÿâ€é¦–æ—¥å°ï¼›7ã€æ˜†å±±é‚®å‹æ–½å…ˆç”Ÿå¯„æ¥çš„â€œå¥¥è¿â€é¦–æ—¥å°ã€‚ï½ä¸€å¹¶è‡´è°¢ó¾Œ¸");
    str.add("ó¾“¶ã€latojaèº«ä½“ä¹³ã€‘ó¾“¶ä¸€æ¬¡å†³å®šæˆè´¥â—â—â—82.7cmåˆ°79.8cmó¾­š");
    str.add("160815 é«˜è‹±åŸ¹ instagram æ›´æ–° å–”å—šå‘€~~~#éƒ½çŸ¥é“ó¾• èŠ±ç±ƒï¼šå®‡å®™çš„ä¸­å¿ƒ soran mbc è—å¤œdjé˜é‰‰*è£½ä½œ cr hello_é‡‘é’Ÿé“‰ä¸­æ–‡ç½‘");
    str.add("è‚è‡Ÿå¥åº·çœŸçš„å¥½é‡è¦ã€‚é›–èªªå®ƒæ˜¯æ²ˆé»˜çš„å™¨å®˜ï¼Œä½†é‚„æ˜¯æœ‰è·¡å¯å¾ªã€‚åƒæ˜¯å£è‡­ã€é»ƒç–¸ã€è¦ºå¾—ç´¯...ç­‰ã€‚å†å¿™é‚„æ˜¯è¦å¥½å¥½ç…§é¡§å°å¿ƒè‚ ó¾Œ¸ó¾Œ¸");
    
    //System.out.println(str7 +"\n"+ analyzer.analyzeText(str7, "en"));
    //System.out.println(str8 +"\n"+ analyzer.analyzeText(str8, "zh"));
    //System.out.println(str9  +"\n"+ analyzer.analyzeText(str9 , "zh"));
    System.out.println(str10 +"\n"+ analyzer.analyzeText(str10, "ja"));
    
    //Set<Set<String>> pw_ngrams = analyzer.powerSet(new HashSet<>(analyzer.analyzeText(str7,"en").keySet()));
    //System.out.println(pw_ngrams.size());
    /*
    System.out.println("\nNGRAMS\n");
    for (Set<String> pw_ngram : pw_ngrams) {
      System.out.println(pw_ngram);
    }
    */
    
  }
  
}
